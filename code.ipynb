{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy (from -r requirements (line 1))Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Ignored the following versions that require a different python version: 1.21.2 Requires-Python >=3.7,<3.11; 1.21.3 Requires-Python >=3.7,<3.11; 1.21.4 Requires-Python >=3.7,<3.11; 1.21.5 Requires-Python >=3.7,<3.11; 1.21.6 Requires-Python >=3.7,<3.11\n",
      "ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)\n",
      "ERROR: No matching distribution found for tensorflow\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Obtaining dependency information for numpy from https://files.pythonhosted.org/packages/16/2e/86f24451c2d530c88daf997cb8d6ac622c1d40d19f5a031ed68a4b73a374/numpy-1.26.4-cp312-cp312-win_amd64.whl.metadata\n",
      "  Using cached numpy-1.26.4-cp312-cp312-win_amd64.whl.metadata (61 kB)\n",
      "Collecting pandas (from -r requirements (line 2))\n",
      "  Obtaining dependency information for pandas from https://files.pythonhosted.org/packages/87/03/fe50521919aa981f6a1c197037da4623a267b0e5f42246d69ba048e86da3/pandas-2.2.0-cp312-cp312-win_amd64.whl.metadata\n",
      "  Using cached pandas-2.2.0-cp312-cp312-win_amd64.whl.metadata (19 kB)\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "import gensim\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import warnings\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Twitter_Data.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"category\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vectorizer = CountVectorizer( stop_words='english')  #max_df=0.90, min_df=2, max_features=1000,\n",
    "bow_lg = bow_vectorizer.fit_transform(df[\"clean_text\"])\n",
    "bow_lg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer( stop_words='english') #max_df=0.90, min_df=2, max_features=1000,\n",
    "tfidf_lg = tfidf_vectorizer.fit_transform(df[\"clean_text\"])\n",
    "tfidf_lg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"category\"] = df[\"category\"].replace(-1, 2)\n",
    "\n",
    "df[\"category\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xtrain_bow, xvalid_bow, ytrain_bow, yvalid_bow = train_test_split(bow_lg, df['category'] , random_state=42, test_size=0.3)\n",
    "\n",
    "\n",
    "lreg = LogisticRegression() \n",
    "# # training the model\n",
    "lreg.fit(xtrain_bow, ytrain_bow) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = lreg.predict_proba(xvalid_bow)\n",
    "test_pred_int = test_pred[:,1] >= 0.3 \n",
    "test_pred_int = test_pred_int.astype(np.int64) \n",
    "\n",
    "pred_bow_lg = lreg.predict(xvalid_bow)\n",
    "\n",
    "# test['label'] = test_pred_int \n",
    "# submission = test[['id','label']] \n",
    "# submission.to_csv('sub_lreg_bow.csv', index=False) # writing data to a CSV file\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "# Accuracy\n",
    "accuracy_bow_lg = accuracy_score(yvalid_bow, pred_bow_lg)\n",
    "print(f'Accuracy: {accuracy_bow_lg:.2f}')\n",
    "\n",
    "# Precision\n",
    "precision_bow_lg = precision_score(yvalid_bow, pred_bow_lg ,  average=\"macro\")\n",
    "print(f'Precision: {precision_bow_lg:.2f}')\n",
    "\n",
    "# Recall\n",
    "recall_bow_lg = recall_score(yvalid_bow, pred_bow_lg  ,  average=\"macro\" )\n",
    "print(f'Recall: {recall_bow_lg:.2f}')\n",
    "\n",
    "# F1 Score\n",
    "f1_bow_lg = f1_score(yvalid_bow, pred_bow_lg   ,  average=\"macro\")\n",
    "print(f'F1 Score: {f1_bow_lg:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = [accuracy_bow_lg, precision_bow_lg, recall_bow_lg, f1_bow_lg]\n",
    "labels = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "\n",
    "plt.bar(labels, values, color=['blue', 'green', 'orange', 'red'])\n",
    "plt.ylim(0, 1)  # Set y-axis limit to 0-1 for better visualization\n",
    "plt.title('Performance Metrics')\n",
    "plt.ylabel('Score')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cm = confusion_matrix(yvalid_bow, pred_bow_lg)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_tfidf = tfidf_lg[:132969,:] \n",
    "# test_tfidf = tfidf[132970:,:] \n",
    "\n",
    "\n",
    "# train_label1 = df['category'][:132969]\n",
    "# test_label1 = df['category'][132970:]\n",
    "\n",
    "xtrain_tfidf, xvalid_tfidf, ytrain_tfidf, yvalid_tfidf = train_test_split(tfidf_lg, df['category'] , random_state=42, test_size=0.3)\n",
    "\n",
    "\n",
    "lreg = LogisticRegression() \n",
    "# # training the model\n",
    "lreg.fit(xtrain_bow, ytrain_bow) \n",
    "\n",
    "lreg.fit(xtrain_tfidf, ytrain_tfidf) \n",
    "prediction = lreg.predict_proba(xvalid_tfidf) \n",
    "prediction_int = prediction[:,1] >= 0.3\n",
    "prediction_int = prediction_int.astype(np.int64) \n",
    "\n",
    "pred_tf_lg = lreg.predict(xvalid_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "accuracy_tf_lg = accuracy_score(yvalid_tfidf, pred_tf_lg)\n",
    "print(f'Accuracy: {accuracy_tf_lg:.2f}')\n",
    "\n",
    "# Precision\n",
    "precision_tf_lg = precision_score(yvalid_tfidf, pred_tf_lg ,  average=\"macro\")\n",
    "print(f'Precision: {precision_tf_lg:.2f}')\n",
    "\n",
    "# Recall\n",
    "recall_tf_lg = recall_score(yvalid_tfidf, pred_tf_lg  ,  average=\"macro\" )\n",
    "print(f'Recall: {recall_tf_lg:.2f}')\n",
    "\n",
    "# F1 Score\n",
    "f1_tf_lg = f1_score(yvalid_tfidf, pred_tf_lg   ,  average=\"macro\")\n",
    "print(f'F1 Score: {f1_tf_lg:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = [accuracy_tf_lg, precision_tf_lg, recall_tf_lg, f1_tf_lg]\n",
    "labels = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "\n",
    "plt.bar(labels, values, color=['blue', 'green', 'orange', 'red'])\n",
    "plt.ylim(0, 1)  # Set y-axis limit to 0-1 for better visualization\n",
    "plt.title('Performance Metrics')\n",
    "plt.ylabel('Score')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cm = confusion_matrix(yvalid_tfidf, pred_tf_lg)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model1 = XGBClassifier().fit(xtrain_bow, ytrain_bow) \n",
    "prediction1 = xgb_model1.predict(xvalid_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "accuracy_bow_xg = accuracy_score(yvalid_bow, prediction1)\n",
    "print(f'Accuracy: {accuracy_bow_xg:.2f}')\n",
    "\n",
    "# Precision\n",
    "precision_bow_xg = precision_score(yvalid_bow, prediction1 ,  average=\"macro\")\n",
    "print(f'Precision: {precision_bow_xg:.2f}')\n",
    "\n",
    "# Recall\n",
    "recall_bow_xg = recall_score(yvalid_bow, prediction1  ,  average=\"macro\" )\n",
    "print(f'Recall: {recall_bow_xg:.2f}')\n",
    "\n",
    "# F1 Score\n",
    "f1_bow_xg = f1_score(yvalid_bow, prediction1   ,  average=\"macro\")\n",
    "print(f'F1 Score: {f1_bow_xg:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = [accuracy_bow_xg, precision_bow_xg, recall_bow_xg, f1_bow_xg]\n",
    "labels = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "\n",
    "plt.bar(labels, values, color=['blue', 'green', 'orange', 'red'])\n",
    "plt.ylim(0, 1)  # Set y-axis limit to 0-1 for better visualization\n",
    "plt.title('Performance Metrics')\n",
    "plt.ylabel('Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cm1 = confusion_matrix(yvalid_bow, prediction1)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(cm1 , annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xgb_model = XGBClassifier().fit(xtrain_tfidf, ytrain_tfidf) \n",
    "prediction2 = xgb_model.predict(xvalid_tfidf) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "accuracy_tf_xg = accuracy_score(yvalid_tfidf, prediction2)\n",
    "print(f'Accuracy: {accuracy_tf_xg:.2f}')\n",
    "\n",
    "# Precision\n",
    "precision_tf_xg  = precision_score(yvalid_tfidf, prediction2 ,  average=\"macro\")\n",
    "print(f'Precision: {precision_tf_xg :.2f}')\n",
    "\n",
    "# Recall\n",
    "recall_tf_xg  = recall_score(yvalid_tfidf, prediction2  ,  average=\"macro\" )\n",
    "print(f'Recall: {recall_tf_xg :.2f}')\n",
    "\n",
    "# F1 Score\n",
    "f1_tf_xg  = f1_score(yvalid_tfidf, prediction2   ,  average=\"macro\")\n",
    "print(f'F1 Score: {f1_tf_xg :.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = [accuracy_tf_xg , precision_tf_xg , recall_tf_xg , f1_tf_xg ]\n",
    "labels = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "\n",
    "plt.bar(labels, values, color=['blue', 'green', 'orange', 'red'])\n",
    "plt.ylim(0, 1)  # Set y-axis limit to 0-1 for better visualization\n",
    "plt.title('Performance Metrics')\n",
    "plt.ylabel('Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cm2 = confusion_matrix(yvalid_tfidf, prediction2)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(cm2, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testr4 = bow_vectorizer.transform([\"I am ok with the recent policies implemented by the government. #PoliticalFrustration\"])\n",
    "\n",
    "lreg.predict(testr4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
